{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":89041,"databundleVersionId":10228459,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\n\nsweep_config = {\n    \"method\": \"grid\",\n    \"metric\": {\n        \"name\": \"val_accuracy\",\n        \"goal\": \"maximize\"\n    },\n    \"parameters\": {\n        \"dataset\": {\"value\": \"CIFAR100\"},\n        \"data_path\": {\"value\": \"/kaggle/input/fii-atnn-2024-project-noisy-cifar-100/fii-atnn-2024-project-noisy-cifar-100\"},\n        \"model_name\": {\"values\": [ \"resnet18_resize\", \"resnet50_resize\"]},\n        \"num_classes\": {\"value\": 100},\n        \"batch_size\": {\"values\": [64]},\n        \"num_epochs\": {\"values\": [100]},\n         # \"learning_rate\": {\n        \"optimizer_config\": {\n        \"values\": [\n            {\"optimizer\": \"adamw\", \"learning_rate\": 0.0005},\n             {\"optimizer\": \"adamw\", \"learning_rate\": 0.001},\n             {\"optimizer\": \"sgd\", \"learning_rate\": 0.01}\n        ]\n    },\n        \"weight_decay\": {\"value\": 0.0005},\n        # \"optimizer\": {\"values\": [\"adamw\", \"sgd\"]},\n        \"momentum\": {\"value\": 0.9},\n        \"nesterov\": {\"value\": True},\n        \"patience\": {\"value\": 3},\n        \"stop_mode\": {\"value\": \"max\"},\n        \"min_delta\": {\"value\": 0.0001},\n        \"scheduler\": {\"values\": [\"cosineannealinglr\"]},\n        \"t_max\": {\"values\": [100]},\n        \"eta_min\": {\"value\": 0.00001},\n        \"augmentation_scheme\": {\"values\": [\"randaugment\", \"combined_resize2\"]},\n        \"use_cutmix\": {\"values\": [True]},\n        \"use_mixup\": {\"values\": [True]},\n        \"alpha\": {\"value\": 1.0},\n        \"t_0\": {\"value\": 10},\n        \"t_mult\": {\"value\": 2},\n        \"warmup\": {\"value\": 5},\n        \"patience_early_stopping\": {\"value\": 5},\n        \"pretrained\": {\"value\": True}\n    }\n}\n\n\nwith open(\"sweep_config.json\", \"w\") as f:\n    json.dump(sweep_config, f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:39:48.336342Z","iopub.execute_input":"2024-12-06T21:39:48.336583Z","iopub.status.idle":"2024-12-06T21:39:48.348151Z","shell.execute_reply.started":"2024-12-06T21:39:48.336557Z","shell.execute_reply":"2024-12-06T21:39:48.347225Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import json\n\nimport numpy as np\nimport yaml\nfrom torch import nn, Tensor\nimport torch.nn.functional as F\nimport random\n\nimport torch.backends.cudnn\n\n\nfrom typing import Literal, cast, Optional, Callable\n\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torchvision.datasets import CIFAR100\nfrom torchvision.transforms.v2 import CutMix, MixUp\n\nfrom torchvision.transforms import v2\nfrom timm import create_model\nimport os\nimport pickle\nfrom torchvision import datasets\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.transforms import AutoAugment, AutoAugmentPolicy\n\nimport torch.optim.lr_scheduler as lr_scheduler\npin_memory = True\n\ndef get_device():\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\ndef cache_dataset(dataset_class, data_dir, cache_dir='./cache', train=True):\n    os.makedirs(cache_dir, exist_ok=True)\n    subset = 'train' if train else 'test'\n    cache_path = os.path.join(cache_dir, f'{dataset_class.__name__}_{subset}.pkl')\n\n    if os.path.exists(cache_path):\n        with open(cache_path, 'rb') as f:\n            data = pickle.load(f)\n    else:\n        data = dataset_class(root=data_dir, train=train, download=True)\n        with open(cache_path, 'wb') as f:\n            pickle.dump(data, f)\n\n    return data\n\n\ndef get_data_augmentation(scheme=\"basic\", dataset=\"CIFAR\"):\n    if dataset == \"CIFAR\":\n        if scheme == \"basic\":\n            train_transform = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n                                          v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n        elif scheme == \"random_flip\":\n            train_transform = v2.Compose(\n                [v2.RandomHorizontalFlip(), v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n                 v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n        elif scheme == \"random_crop_flip\":\n            train_transform = v2.Compose([v2.RandomCrop(32, padding=4), v2.RandomHorizontalFlip(), v2.ToImage(),\n                                          v2.ToDtype(torch.float32, scale=True),\n                                          v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n        elif scheme == \"randaugment\":\n            train_transform = v2.Compose(\n                [v2.RandAugment(), v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n                 v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n        elif scheme == \"autoaugment\":\n            train_transform = v2.Compose(\n                [AutoAugment(policy=AutoAugmentPolicy.CIFAR10), v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n                 v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n\n        elif scheme == \"combined\":\n            train_transform = v2.Compose(\n                [v2.RandomResizedCrop(32, scale=(0.8, 1.0)), v2.RandomHorizontalFlip(), v2.RandomRotation(15),\n                 v2.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1), v2.RandAugment(), v2.ToImage(),\n                 v2.ToDtype(torch.float32, scale=True),\n                 v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n        elif scheme == \"combined2\":\n            train_transform = v2.Compose(\n                [AutoAugment(policy=AutoAugmentPolicy.CIFAR10), v2.RandomCrop(32, padding=4), v2.RandomHorizontalFlip(),\n                 v2.ColorJitter(brightness=0.2, contrast=0.2),\n                 v2.RandomRotation(15), v2.AutoAugment(), v2.ToImage(), v2.ToDtype(torch.float32, scale=True),\n                 v2.Normalize((0.5,), (0.5,))])\n        elif scheme == \"combined_resize\":\n            train_transform = v2.Compose([\n                v2.Resize((64, 64)),\n                v2.RandomResizedCrop(64, scale=(0.8, 1.0)),\n                v2.RandomRotation(15),\n                v2.RandomHorizontalFlip(),\n                v2.RandAugment(),\n                v2.ToImage(),\n                v2.ToDtype(torch.float32, scale=True),\n                v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n            ])\n        elif scheme == \"combined_resize2\":\n            train_transform = v2.Compose([\n                v2.RandomRotation(10),\n                v2.RandomResizedCrop(32, scale=(0.9, 1.1)),\n                v2.RandomHorizontalFlip(),\n                v2.RandomAffine(degrees=0, shear=10),\n                v2.RandomCrop(32, padding=3),\n                v2.ToImage(),\n                v2.ToDtype(torch.float32, scale=True),\n                v2.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n            ])\n        else:\n            raise ValueError(f\"Augmentation scheme '{scheme}' not supported for CIFAR.\")\n        test_transform = v2.Compose(\n            [v2.ToImage(), v2.ToDtype(torch.float32, scale=True), v2.Normalize((0.5, 0.5, 0.5), (0.25, 0.25, 0.25))])\n\n    else:\n        raise ValueError(f\"Dataset '{dataset}' not supported.\")\n\n    return train_transform, test_transform\n\nclass SimpleCachedDataset(Dataset):\n    def __init__(self, dataset):\n        # Runtime transforms are not implemented in this simple cached dataset.\n        self.data = tuple([x for x in dataset])\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, i):\n        return self.data[i]\n\n\nclass CIFAR100_noisy_fine(Dataset):\n    \"\"\"\n    See https://github.com/UCSC-REAL/cifar-10-100n, https://www.noisylabels.com/ and `Learning with Noisy Labels\n    Revisited: A Study Using Real-World Human Annotations`.\n    \"\"\"\n\n    def __init__(\n        self, root: str, train: bool, transform: Optional[Callable], download: bool\n    ):\n        cifar100 = CIFAR100(\n            root=root, train=train, transform=transform, download=download\n        )\n        data, targets = tuple(zip(*cifar100))\n\n        if train:\n            noisy_label_file = os.path.join(root, \"CIFAR-100-noisy.npz\")\n            if not os.path.isfile(noisy_label_file):\n                raise FileNotFoundError(\n                    f\"{type(self).__name__} need {noisy_label_file} to be used!\"\n                )\n\n            noise_file = np.load(noisy_label_file)\n            if not np.array_equal(noise_file[\"clean_label\"], targets):\n                raise RuntimeError(\"Clean labels do not match!\")\n            targets = noise_file[\"noisy_label\"]\n\n        self.data = data\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, i: int):\n        return self.data[i], self.targets[i]\n\ndef load_data( batch_size=64, scheme=\"basic\", custom_transforms=None):\n    if custom_transforms:\n        train_transform, test_transform = custom_transforms\n    else:\n        train_transform, test_transform = get_data_augmentation(scheme=scheme, dataset=\"CIFAR\")\n\n    try:\n        train_set = CIFAR100_noisy_fine(\n            '/kaggle/input/fii-atnn-2024-project-noisy-cifar-100/fii-atnn-2024-project-noisy-cifar-100', download=False,\n            train=True, transform=train_transform)\n        test_set = CIFAR100_noisy_fine(\n            '/kaggle/input/fii-atnn-2024-project-noisy-cifar-100/fii-atnn-2024-project-noisy-cifar-100', download=False,\n            train=False, transform=test_transform)\n    except Exception as e:\n        print(f\"Error loading dataset: {e}\")\n        raise\n\n    train_set.transform = train_transform\n    test_set.transform = test_transform\n\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, pin_memory=pin_memory)\n    test_loader = DataLoader(test_set, batch_size=500, pin_memory=pin_memory)\n\n    return train_loader, test_loader\n\n\ndef get_model(dataset, model_name, num_classes, input_size=None, hidden_layers=None, pretrained=True):\n    if dataset == 'CIFAR10' or dataset == 'CIFAR100':\n        if model_name in ['resnet18', 'resnet18_resize']:\n            model = create_model(\"resnet18\", pretrained=pretrained, num_classes=num_classes)\n        elif model_name in ['resnet50', 'resnet50_resize']:\n            model = create_model(\"resnet50\", pretrained=pretrained, num_classes=num_classes)\n        elif model_name == 'resnet18_cifar10':\n            model = create_model(\"hf_hub:edadaltocg/resnet18_cifar10\", pretrained=False, num_classes=num_classes)\n            model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n\n        else:\n            raise ValueError(\n                f\"Model '{model_name}' is not supported for CIFAR.\")\n        if pretrained and model_name in ['resnet18_resize', 'resnet50_resize']:\n            model = nn.Sequential(\n                nn.Upsample(size=(224, 224), mode='bilinear', align_corners=False),\n                model\n            )\n\n\n    else:\n        raise ValueError(f\"Dataset '{dataset}' is not supported. Choose 'CIFAR10', 'CIFAR100', or 'MNIST'.\")\n\n    return model\n\n\nimport torch.optim as optim\n\n\ndef get_optimizer(optimizer_name, model_parameters, lr=0.001, momentum=0.9, weight_decay=0.0, nesterov=True):\n    optimizer_name = optimizer_name.lower()\n\n    if optimizer_name == 'sgd':\n        return optim.SGD(model_parameters, lr=lr)\n    elif optimizer_name == 'sgd_momentum':\n        return optim.SGD(model_parameters, lr=lr, momentum=momentum)\n    elif optimizer_name == 'sgd_nesterov':\n        return optim.SGD(model_parameters, lr=lr, momentum=momentum, nesterov=nesterov)\n    elif optimizer_name == 'sgd_weight_decay':\n        return optim.SGD(model_parameters, lr=lr, momentum=momentum, weight_decay=weight_decay)\n    elif optimizer_name == 'adam':\n        return optim.Adam(model_parameters, lr=lr, weight_decay=weight_decay)\n    elif optimizer_name == 'adamw':\n        return optim.AdamW(model_parameters, lr=lr, weight_decay=weight_decay)\n    elif optimizer_name == 'rmsprop':\n        return optim.RMSprop(model_parameters, lr=lr, momentum=momentum, weight_decay=weight_decay)\n    else:\n        raise ValueError(f\"Optimizer '{optimizer_name}' not supported.\")\n\n\ndef get_scheduler(optimizer, scheduler_name, **kwargs):\n    scheduler_name = scheduler_name.lower()\n\n    if scheduler_name == 'steplr':\n        return lr_scheduler.StepLR(optimizer, step_size=kwargs.get('step_size', 10), gamma=kwargs.get('gamma', 0.1))\n\n    elif scheduler_name == 'reducelronplateau':\n        mode_str = kwargs.get('mode', 'min')\n        if mode_str not in ['min', 'max']:\n            raise ValueError(\"Invalid mode for ReduceLROnPlateau: must be 'min' or 'max'\")\n        mode = cast(Literal[\"min\", \"max\"], mode_str)\n        return lr_scheduler.ReduceLROnPlateau(\n            optimizer,\n            mode=mode,\n            factor=kwargs.get('factor', 0.1),\n            patience=kwargs.get('patience', 10)\n        )\n\n    elif scheduler_name == 'cosineannealinglr':\n        return lr_scheduler.CosineAnnealingLR(\n            optimizer,\n            T_max=kwargs.get('t_max', 50),\n            eta_min=kwargs.get('eta_min', 0)\n        )\n\n    elif scheduler_name == 'cosineannealingwarmrestarts':\n        return lr_scheduler.CosineAnnealingWarmRestarts(\n            optimizer,\n            T_0=kwargs.get('t_0', 10),\n            T_mult=kwargs.get('t_mult', 1),\n            eta_min=kwargs.get('eta_min', 0)\n        )\n\n    elif scheduler_name == 'exponentiallr':\n        return lr_scheduler.ExponentialLR(optimizer, gamma=kwargs.get('gamma', 0.9))\n\n    elif scheduler_name == 'linearlr':\n        return lr_scheduler.LinearLR(\n            optimizer,\n            start_factor=kwargs.get('start_factor', 1.0),\n            end_factor=kwargs.get('end_factor', 0.0),\n            total_iters=kwargs.get('total_iters', 100)\n        )\n\n    elif scheduler_name == 'none':\n        return None\n\n    else:\n        raise ValueError(f\"Scheduler '{scheduler_name}' not supported.\")\n\n\ndef early_stopping(current_score, best_score, patience_counter, patience_early_stopping, min_delta=0.0, mode=\"max\"):\n    print(\"heeei\", best_score, current_score, mode, patience_counter)\n    if best_score is None:\n        best_score = current_score\n        return False, best_score, patience_counter\n\n    if mode == \"min\":\n        improvement = best_score - current_score > min_delta\n    elif mode == \"max\":\n        improvement = current_score - best_score > min_delta\n    else:\n        raise ValueError(\"Mode should be 'min' or 'max'\")\n\n    if improvement:\n        best_score = current_score\n        patience_counter = 0\n    else:\n        patience_counter += 1\n\n    early_stop = patience_counter >= patience_early_stopping\n    return early_stop, best_score, patience_counter\n\n\n@torch.inference_mode()\ndef validate_model(model, test_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct, total = 0, 0\n\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_loss = running_loss / len(test_loader.dataset)\n    val_accuracy = 100 * correct / total\n    return val_loss, val_accuracy\n\n\nfrom torch.amp import autocast, GradScaler\n\n\ndef train_model(model, train_loader, test_loader, device, num_epochs, optimizer, num_classes, scheduler_mode=None,\n                scheduler=None, patience_early_stopping=5, min_delta=0.0, early_stop_mode=\"max\", learning_rate=0.1,\n                warmup=0, grad_alpha=1.0, use_cutmix=True, use_mixup=True, alpha=1.0):\n    model = model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    scaler = GradScaler(device)\n\n    best_val_score = None\n    best_train_score = None\n    patience_counter = 0\n    best_val_accuracy = 0.0\n    best_train_accuracy = 0.0\n    best_val_loss = 100.0\n    best_train_loss = 100.0\n    wandb.watch(model, log=\"all\", log_freq=10)\n    alpha = float(alpha)\n    cutmix = v2.CutMix(num_classes=num_classes, alpha=alpha)\n    mixup = v2.MixUp(num_classes=num_classes, alpha=alpha)\n    cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])\n    rand = random.randint(1000, 9999)\n    file_path = f\"/kaggle/working/best_model_{rand}.pth\"\n    print(\"saving\", file_path)\n    for epoch in range(num_epochs):\n        print(\"Epoch \", epoch)\n        model.train()\n        running_loss = 0.0\n        correct, total = 0, 0\n\n        if epoch < warmup:\n            lr_scale = min(1., float(epoch + 1) / warmup)\n            for pg in optimizer.param_groups:\n                pg['lr'] = learning_rate * lr_scale\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n            if use_cutmix and use_mixup:\n                inputs, labels = cutmix_or_mixup(inputs, labels)\n            elif use_cutmix:\n                inputs, labels = cutmix(inputs, labels)\n            elif use_mixup:\n                inputs, labels = mixup(inputs, labels)\n\n            optimizer.zero_grad()\n            with autocast(device):\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n            scaler.scale(loss).backward()\n\n            #             scaler.unscale_(optimizer)\n            #             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=grad_alpha)\n\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item() * inputs.size(0)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            if use_cutmix or use_mixup:\n                correct += predicted.eq(labels.argmax(dim=1)).sum().item()\n            else:\n                correct += predicted.eq(labels).sum().item()\n\n        train_loss = running_loss / len(train_loader.dataset)\n        train_accuracy = 100 * correct / total\n\n        val_loss, val_accuracy = validate_model(model, test_loader, criterion, device)\n        if train_accuracy > best_train_accuracy:\n            best_train_accuracy = train_accuracy\n        if train_loss < best_train_loss:\n            best_train_loss = train_loss\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n        if val_accuracy > best_val_accuracy:\n            best_val_accuracy = val_accuracy\n            torch.save(model.state_dict(), file_path)\n            print(f\"Best model saved with accuracy: {best_val_accuracy:.2f}%\")\n        print(f\"Epoch {epoch + 1}/{num_epochs} - \")\n        print(f\"Train Loss: {train_loss:.4f} - Train Accuracy: {train_accuracy:.2f}% - \")\n        print(f\"Val Loss: {val_loss:.4f} - Val Accuracy: {val_accuracy:.2f}%\")\n        wandb.log({\n            'epoch': epoch + 1,\n            'train_loss': train_loss,\n            'train_accuracy': train_accuracy,\n            'val_loss': val_loss,\n            'val_accuracy': val_accuracy\n        })\n\n        val_score = val_loss if early_stop_mode == \"min\" else val_accuracy\n        train_score = train_loss if early_stop_mode == \"min\" else train_accuracy\n\n       \n\n        if isinstance(scheduler, lr_scheduler.ReduceLROnPlateau):\n            if scheduler_mode == \"max\":\n                scheduler.step(val_accuracy)\n            elif scheduler_mode == \"min\":\n                scheduler.step(val_loss)\n        elif scheduler:\n            scheduler.step()\n\n        early_stop, best_train_score, patience_counter = early_stopping(\n            current_score=train_score,\n            best_score=best_train_score,\n            patience_counter=patience_counter,\n            patience_early_stopping=patience_early_stopping,\n            min_delta=min_delta,\n            mode=early_stop_mode\n        )\n\n        if early_stop:\n            print(\"Early stopping triggered. Stopping training.\")\n            break\n        # best_val_score =  best_val_loss if early_stop_mode == \"min\" else  best_val_accuracy\n        # best_train_score =  best_train_loss if early_stop_mode == \"min\" else  best_train_accuracy\n    print(\"Training complete.\")\n\n\nimport wandb\n\n\ndef sweep_train():\n    wandb.init()\n    config = wandb.config\n    try:\n        train_loader, test_loader = load_data(\n            batch_size=config.batch_size,\n            scheme=config.augmentation_scheme,\n        )\n\n        model = get_model(\n            dataset=config.dataset,\n            model_name=config.model_name,\n            num_classes=config.num_classes\n        )\n        optimizer_name = config.optimizer_config[\"optimizer\"]\n        learning_rate = config.optimizer_config[\"learning_rate\"]\n\n        optimizer = get_optimizer(\n            optimizer_name=optimizer_name,\n            model_parameters=model.parameters(),\n            lr=learning_rate,\n            momentum=config.momentum,\n            weight_decay=config.weight_decay,\n            nesterov=config.nesterov\n        )\n\n        scheduler = get_scheduler(\n            optimizer=optimizer,\n            scheduler_name=config.scheduler,\n            t_max=config.get('t_max', 200),\n            eta_min=config.get('eta_min', 0),\n            step_size=config.get('step_size', 10),\n            gamma=config.get('gamma', 0.1),\n            patience=config.get('scheduler_patience', 10),\n            factor=config.get('factor', 0.1)\n        )\n\n        train_model(\n            model=model,\n            train_loader=train_loader,\n            test_loader=test_loader,\n            device=get_device(),\n            num_epochs=config.num_epochs,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            patience_early_stopping=config.patience_early_stopping,\n            min_delta=config.min_delta,\n            early_stop_mode=config.stop_mode,\n            learning_rate=learning_rate,\n            num_classes=config.num_classes,\n            use_cutmix=config.use_cutmix,\n            use_mixup=config.use_mixup,\n            alpha=config.alpha,\n            warmup=config.warmup,\n        )\n    finally:\n        wandb.finish()\n\n\ndef load_config(file_path):\n    ext = os.path.splitext(file_path)[-1].lower()\n    if ext == \".json\":\n        with open(file_path, 'r') as f:\n            config = json.load(f)\n    elif ext in {\".yaml\", \".yml\"}:\n        with open(file_path, 'r') as f:\n            config = yaml.safe_load(f)\n    else:\n        raise ValueError(\"Unsupported file format. Use JSON or YAML.\")\n    return config\n\n\nconfig_file_path = \"sweep_config.json\"\nsweep_config = load_config(config_file_path)\n\nsweep_id = wandb.sweep(sweep_config, project=\"training-cifar100-noisy\")\nwandb.agent(sweep_id, sweep_train)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-06T21:39:48.375808Z","iopub.execute_input":"2024-12-06T21:39:48.376077Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: e31mdz1a\nSweep URL: https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100-noisy/sweeps/e31mdz1a\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0s0bqid9 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \taugmentation_scheme: randaugment\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_path: /kaggle/input/fii-atnn-2024-project-noisy-cifar-100/fii-atnn-2024-project-noisy-cifar-100\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: CIFAR100\n\u001b[34m\u001b[1mwandb\u001b[0m: \teta_min: 1e-05\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmin_delta: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmodel_name: resnet18_resize\n\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.9\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnesterov: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_classes: 100\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 100\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer_config: {'learning_rate': 0.0005, 'optimizer': 'adamw'}\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpatience_early_stopping: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpretrained: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tscheduler: cosineannealinglr\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstop_mode: max\n\u001b[34m\u001b[1mwandb\u001b[0m: \tt_0: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \tt_max: 100\n\u001b[34m\u001b[1mwandb\u001b[0m: \tt_mult: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_cutmix: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \tuse_mixup: True\n\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.0005\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgheorghitastefana\u001b[0m (\u001b[33mgheorghitastefana-alexandru-ioan-cuza-university-iasi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112940299997515, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d874b51b14e946538e101d4203301ade"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241206_214003-0s0bqid9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100-noisy/runs/0s0bqid9' target=\"_blank\">royal-sweep-1</a></strong> to <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100-noisy' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100-noisy/sweeps/e31mdz1a' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100-noisy/sweeps/e31mdz1a</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100-noisy' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100-noisy</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100-noisy/sweeps/e31mdz1a' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100-noisy/sweeps/e31mdz1a</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100-noisy/runs/0s0bqid9' target=\"_blank\">https://wandb.ai/gheorghitastefana-alexandru-ioan-cuza-university-iasi/training-cifar100-noisy/runs/0s0bqid9</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6bbfd42be414b2587697bba176ddb89"}},"metadata":{}},{"name":"stdout","text":"saving /kaggle/working/best_model_7423.pth\nEpoch  0\nBest model saved with accuracy: 25.44%\nEpoch 1/100 - \nTrain Loss: 4.3464 - Train Accuracy: 7.10% - \nVal Loss: 3.2486 - Val Accuracy: 25.44%\nheeei None 7.102 max 0\nEpoch  1\nBest model saved with accuracy: 53.15%\nEpoch 2/100 - \nTrain Loss: 3.5610 - Train Accuracy: 24.57% - \nVal Loss: 1.9561 - Val Accuracy: 53.15%\nheeei 7.102 24.574 max 0\nEpoch  2\n","output_type":"stream"}],"execution_count":null}]}